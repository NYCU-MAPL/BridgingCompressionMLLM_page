<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bridging Compressed Image Latents and Multimodal Large Language Models</title>
  <script type="text/javascript" src="assets/latexit.js"></script>
  <script type="text/javascript">
    LatexIT.add('p', true);
  </script>

  <!-- CSS includes -->
  <link href="assets/bootstrap.css" rel="stylesheet">
  <link href="assets/css.css" rel="stylesheet" type="text/css">
  <link href="assets/mystyle.css" rel="stylesheet">
  <link href="assets/lightbox2-2.11.3/dist/css/lightbox.css" rel="stylesheet" />

  <link rel="icon" href="assets/icon/bridge.png" type="image/x-icon">
</head>

<body>

  <div id="header" class="container-fluid">
    <div class="row">
      <h1><img src="assets/icon/bridge.png" height="55"> Bridging Compressed Image Latents and <br /> Multimodal Large Language Models</h1>
      <div class="authors">
        <a href="https://joek6279.github.io/">Chia-Hao Kao</a>, <a href="https://jamesqian1999.github.io/">Cheng Chien</a>,
         <a>Yu-Jen Tseng</a>, <a>Yi-Hsin Chen</a>,
         <br /> <a href="https://scholar.google.com/citations?user=m57EUcoAAAAJ&hl=it"> Alessandro Gnutti</a>, <a href="https://shaoyuanlo.github.io/">Shao-Yuan Lo</a>, 
         <a href="https://sites.google.com/g2.nctu.edu.tw/wpeng/cv">Wen-Hsiao Peng</a>, <a href="https://scholar.google.com/citations?user=nojVrZgAAAAJ&hl=zh-TW"> Riccardo Leonardi</a>
      </div>
      <div class="conference">
        <a href="https://iclr.cc/">ICLR 2025</a>    
        <span style="display:inline-block; width: 5px;"></span>
        <a href="https://arxiv.org/abs/2407.19651" target="_blank"><img src="assets/icon/arxiv.jpg" height="35"></a>
        <!-- <span style="display:inline-block; width: 1px;"></span> -->
        <a href="https://github.com/NYCU-MAPL/BridgingCompressionMLLM" target="_blank"><img src="assets/icon/GitHub-Mark.png" height="45"></a>
      </div>
    </div>

    <p style="text-align:center;">
      <a href="https://en.nycu.edu.tw/" target="_blank"><img src="assets/icon/nycu_b.png" height="230"></a>
      <a href="https://mapl.cs.nycu.edu.tw/content/index.html" target="_blank"><img src="assets/icon/mapl_logo.png" height="120"></a>
      <br>
      <a href="https://www.unibs.it/en" target="_blank"><img src="assets/icon/ENG_marchio Unibs orizzontale PANTONE287U.png" height="120"  style="padding-right: 200px;"></a>
      <a href="https://usa.honda-ri.com/" target="_blank"><img src="assets/icon/honda-main-logo.png" height="110"></a>
    </p>
  </div>


  <div class="container" id="abstractdiv">
    <h2>Abstract</h2>
    <p>
      This paper presents the first-ever study of adapting compressed image latents to
      suit the needs of downstream vision tasks that adopt Multimodal Large Language
      Models (MLLMs). MLLMs have extended the success of large language models
      to modalities (e.g. images) beyond text, but their billion scale hinders deployment on resource-constrained end devices. While cloud-hosted MLLMs could be
      available, transmitting raw, uncompressed images captured by end devices to the
      cloud requires an efficient image compression system. To address this, we focus
      on emerging neural image compression and propose a novel framework with a
      lightweight transform-neck and a surrogate loss to adapt compressed image latents
      for MLLM-based vision tasks. Given the huge scale of MLLMs, our framework excludes the entire downstream MLLM except part of its visual encoder from training
      our system. This stands out from most existing coding for machine approaches that
      involve downstream networks in training and thus could be impractical when the
      networks are MLLMs. The proposed framework is general in that it is applicable to
      various MLLMs, neural image codecs, and multiple application scenarios, where
      the neural image codec can be (1) pre-trained for human perception without updating, (2) fully updated for joint human and machine perception, or (3) fully updated
      for only machine perception. Extensive experiments on different neural image
      codecs and various MLLMs show that our method achieves great rate-accuracy
      performance with much less complexity.
    </p>
    <div class="col-md-12">
      <hr>
    </div>
  </div>



  <div class="container" id="banner">
    <h2>Method</h2>
    <p style="text-align:center;">
      <a href="assets/architecture.png" data-lightbox="arch"><img src="assets/architecture.png" data-lightbox="arch"
          width="95%"></a>
    </p>
    <br>
    <p>
      The proposed method introduces a novel approach to adapting compressed image latents for Multimodal Large Language Models (MLLMs), 
      addressing the challenge of efficiently transmitting images while preserving task performance. Instead of reconstructing images from 
      compressed latents, the method employs a lightweight transform-neck module that directly adapts the latents to the intermediate features of 
      the MLLM’s visual encoder. This avoids computationally expensive image reconstruction, reducing complexity while maintaining recognition accuracy. 
      Additionally, a surrogate loss function—combining cross-entropy and distillation loss—guides the adaptation process, 
      ensuring that transformed latents align well with the visual encoder’s feature space. The method is applicable to various application scenarios 
      where the image codec could be (1) pre-trained and fixed for human perception (2) jointly optimized for human and machine visions 
      (3) specifically optimized for MLLMs.
    </p>
    <div class="col-md-12">
      <hr>
    </div>
  </div>

    <div class="container" id="exp_results">
      <h2>Rate-distortion Results</h2>
      <p>
        The proposed method significantly improves rate-accuracy performance for MLLM-based vision tasks by efficiently adapting compressed image latents. 
        Compared to using reconstructed images from codecs optimized for human perception,  
        it achieves up to 60-80% bit-rate reductions at the same accuracy performance. Compared with image post-processing baseline, the lightweight transform-neck achieves competitive performance while reducing decoding complexity by nearly 95% in terms of multiply-accumulate operations (kMAC/pixel). The method generalizes well across different MLLMs, neural image codecs, and tasks, demonstrating superior performance
        By allowing the encoder to be optimizeding for machine perception (d2, d3), the approach achieves recognition performance accuracy closer to uncompressed images while reducing transmission cost.        </p>
      <br>

      <div id="exp1">
        <div class="col-md-6 vcenter">
          <p style="text-align:center;">
            <a href="assets/results/paper_main_RD_full.png" data-lightbox="Exp1"><img
                src="assets/results/paper_main_RD_full.png" width="1080"></a>
          </p>
        </div>
      </div>
      <div class="col-md-12">
        <hr>
      </div>
    </div>

    <div class="container" id="Visualization">
      <h2>Qualitative Comparison</h2>
      <p>
        The visualizations highlight the effectiveness of the proposed method in image captioning, visual question answering (VQA), 
        referring expression comprehension (REC), and few-shot classification. 
        Compared to standard reconstruction and post-processing baselines which show different artifacts in reconstructed images, 
        our proposed method consistently generates more accurate and semantically meaningful outputs, even at low bitrates without 
        the need for image reconstruction. 
      </p>

      <div id="exp1">
        <div class="col-md-6 vcenter">
          <p style="text-align:center;">
            <a href="assets/results/visualize_cap.png" data-lightbox="Exp1"><img
              src="assets/results/visualize_cap.png" width="1080"></a>
          </p>
        </div>
      </div>

      <div class="col-md-12">
        <hr>
      </div>
    </div>

    <div id=footer><br></div>
    <!-- Javascript includes -->
    <script src="assets/jquery-1.js"></script>
    <script src="assets/bootstrap.js"></script>
    <script src="assets/lightbox2-2.11.3/dist/js/lightbox.js"></script>


</body>

</html>